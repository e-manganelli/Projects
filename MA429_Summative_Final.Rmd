---
title: "MA429_Project_Final"
output: html_document
date: "2024-03-25"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
options("install.lock"=FALSE) 
```

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(tree)
library(rpart)
library(randomForest)
library(caret)
library(rpart.plot)
library(VIM)
```

# File Import
```{r}
## Importing df

df <- read.csv("diabetic_data.csv") # change accordingly 
nrow(df)

```

## Pre-Processing

## Missing value analysis
```{r}
# Understanding missing values

missing_values = sapply(df, function(x) any(x == "?")) # missing values denoted by "?"

# Variables with missing values:
names(df[missing_values])

# Proportion of datapoints missing in each variable
for (i in names(df)[missing_values]) {
  print(i)
  print(head(prop.table(table(df[[i]]))*100))
}
# highest:
# "weight" ?  % 96.85847926
# "payer_code" ? % 39.5574160
# "medical_specialty" ? % 49.082208203 

## Eliminating variables with high percentage of missing values
df = df[, -which(names(df) == "weight")]
df = df[, -which(names(df) == "payer_code")]
df = df[, -which(names(df) == "medical_specialty")]
```

## Deleting rows with certain outcomes (death/hospice) guaranteeing no return
```{r}
# Creating a function to delete rows with outcome == 11, 13, 14, 19, 20, 21 as 
# they would increase bias (dead, hospice etc.) ----

delete_rows <- function(data, outcome_number) {
  # rows' index
  indices_to_delete <- which(outcome_number %in% c('11', '13', '14', '19', '20', '21'))
  
  # deletion
  data <- data[-indices_to_delete, ]
  
  return(data)
}

# Deleting rows with outcome = death/hospice

# nrow(df) # 101766 before

df <- delete_rows(df, df$discharge_disposition_id)

# nrow(df) # 99343 after
```

## Categorising diagnosis
```{r}
# Joining types of diagnosis in common categories for simplicity based on ICD9
# codes

# table(df$diag_1) # Uncomment to see format of diagnosis columns as ICD9 codes

# We want to group the diagnosis by type depending on their code, classification 
# taken from literature

classify_diag <- function(diag) {
  
  # record the missing value indices, avoid mixing with elements with letters when transform
  missing_indices <- which(diag == "?")
  
  diag = as.numeric(diag)
  classified <- case_when(
    (diag >= 390 & diag <= 459) | (diag == 785) ~ "circulatory",
    (diag >= 460 & diag <= 519) | (diag == 786) ~ "respiratory",
    (diag >= 520 & diag <= 579) | (diag == 787) ~ "digestive",
    (diag >= 250 & diag < 251) ~ "diabetes",
    (diag >= 800 & diag <= 999) ~ "injury",
    (diag >= 710 & diag <= 739) ~ "musculoskeletal",
    (diag >= 580 & diag <= 629) | (diag == 788) ~ "genitourinary",
    (diag >= 140 & diag <= 239) | (diag == 780) | (diag == 781) | (diag == 784) | 
      (diag >= 790 & diag <= 799) | (diag >= 240 & diag <= 249) | 
      (diag >= 251 & diag <= 279) ~ "neoplasms",
    TRUE ~ "other")
  
  
  classified[missing_indices] = "?"
  calssified = as.factor(classified)
  return(classified)
}

# Categorising the diagnosis columns

df$diag_1 <- classify_diag(df$diag_1)
df$diag_2 <- classify_diag(df$diag_2)
df$diag_3 <- classify_diag(df$diag_3)
```

## Making patients unique and de-identifying
```{r}
# Keeping only 1st patient encounter

df <- df %>%
  group_by(patient_nbr) %>%
  slice(1) %>%
  ungroup()
```

## Removing variables with only one level and identifier variables
```{r}
sapply(df, function(x) length(levels(factor(x))))

df = df[, -which(names(df) == "citoglipton")]
df = df[, -which(names(df) == "examide")]
df = df[, -which(names(df) == "glimepiride.pioglitazone")]
df = df[, -which(names(df) == "encounter_id")]
df = df[, -which(names(df) == "patient_nbr")]

```

# Replacing ? Missing Values with NA
```{r}
# which features have missing values
missing_values = sapply(df, function(x) any(x == "?"))

# Substituting  "?" into NA
df[df == "?"] <- NA

# Replace NA with "Not Available" factor

df <- df %>%
  mutate_all(~ replace(., is.na(.), "Not Available"))

```


## Categorising features as factor/numeric
```{r}
# Transform all features that are character into factor

for (col in names(df)) {
  if (class(df[[col]]) == "character") {
    df[[col]] <- as.factor(df[[col]])
  }
}

# Converting all numeric variables
df$time_in_hospital <- as.numeric(as.character(df$time_in_hospital))
df$num_lab_procedures <- as.numeric(as.character(df$num_lab_procedures))
df$num_procedures <- as.numeric(as.character(df$num_procedures))
df$num_medications <- as.numeric(as.character(df$num_medications))
df$number_outpatient <- as.numeric(as.character(df$number_outpatient))
df$number_emergency <- as.numeric(as.character(df$number_emergency))
df$number_inpatient <- as.numeric(as.character(df$number_inpatient))
df$number_diagnoses <- as.numeric(as.character(df$number_diagnoses))


## Renaming the dependent variable to TRUE/FALSE in answer to question: "Does the patient re-admit?" ----
df$readmitted <- (df$readmitted != "NO") # labels any point with re-admission as TRUE
df$readmitted <- factor(df$readmitted) # Identify readmitted as a factor

```

## Interaction analysis 

The reason for analysing how different variables interact with each other is:
 - To enhance our understanding of how different factors contribute to diabetes
 - Help us with feature selection and dimensionality reduction.
      - We use only the most informative variables

We want to analyse how each variable interacts with the target (readmission).

response variable = "readmitted"
predictor = every other column

```{r}
p_values <- data.frame(
  Variable = rep("", (ncol(df)-1)),
  P_value = rep("", (ncol(df)-1))
)

for (i in 1:(ncol(df)-1)){
  
predictor = names(df[,i])  
formula <- paste("readmitted ~", predictor)

model <- glm(formula, data = df, family = binomial(link = "logit"))

# Fit reduced model without race variable
reduced_model <- glm(readmitted ~ 1, data = df, family = binomial(link = "logit"))

# Perform likelihood ratio test (comparing full model to reduced model)
lr_test <- anova(model, reduced_model, test = "Chisq")


# Add p-value to table
p_values[i,1] = names(df)[i]
p_values[i,2] = lr_test$"Pr(>Chi)"[2] # p-value of likelihood ratio test

p_values[,2] = as.numeric(p_values[,2])
}
```

Interpreting table: 

Variables that give a p-value <0.05 in the likelihood ratio test are statistically 
significant, meaning we have reason to reject H0 and we conclude that this predictor
significantly improves the fit of the model.

Alternatively, all variables that have a p-value >0.05 in the likelihood ratio test 
don't provide enough evidence to reject H0 so we cannot conclude that this variable 
improves the model fit

# Removing variables with p-value above 0.05
```{r}
insignificant_var = which(p_values[,2]>0.05) # = c(22,23,24,25,27,28,32,33,34,36,37,38,39)

# ncol(df) # number of predictors before = 41

df = df[,-insignificant_var]

# ncol(df) # number of predictor after = 28
```

Based on the analysis of the relationship between each variable and the outcome,
we remove 13 variables which are said to be insignificant.


## Training set and test set division
```{r}
# Setting seeds to ensure repeatable results
set.seed(1)

index <- sample(nrow(df), size = nrow(df)*0.9, replace = FALSE)

train <- df[index, ]
test <- df[-index, ]
```
